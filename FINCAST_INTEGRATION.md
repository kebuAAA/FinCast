# FinCast æ—¶åºå¤§æ¨¡å‹é›†æˆæŒ‡å—

## ğŸ“‹ æ¨¡å‹ä¿¡æ¯

**FinCast** æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºé‡‘èæ—¶åºé¢„æµ‹çš„åŸºç¡€å¤§æ¨¡å‹ï¼Œç‰¹ç‚¹ï¼š

- **æ¶æ„**: Decoder-only Transformer
- **è®­ç»ƒæ•°æ®**: è¶…è¿‡200äº¿é‡‘èæ—¶åºæ•°æ®ç‚¹
- **æ ¸å¿ƒæŠ€æœ¯**: 
  - PQ-Loss: è”åˆç‚¹é¢„æµ‹å’Œæ¦‚ç‡é¢„æµ‹
  - Mixture-of-Experts (MoE): è·¨é¢†åŸŸä¸“ä¸šåŒ–
- **é¢„è®­ç»ƒæƒé‡**: `v1.pth` (å·²ä¸‹è½½)
- **å®˜æ–¹ä»“åº“**: https://github.com/vincent05r/FinCast-fts
- **Hugging Face**: https://huggingface.co/Vincent05R/FinCast

## ğŸ”§ é›†æˆæ­¥éª¤

### æ­¥éª¤1: å…‹éš†å®˜æ–¹ä»£ç 

```bash
cd /Users/kobal/Library/CloudStorage/OneDrive-s3wh/æ¯•ä¸šè®¾è®¡/å»ºæ¨¡/src/models/FinCast

# å…‹éš†å®˜æ–¹ä»“åº“
git clone https://github.com/vincent05r/FinCast-fts.git

# æˆ–è€…å¦‚æœå·²ç»ä¸‹è½½ï¼Œå°†ä»£ç å¤åˆ¶åˆ°FinCastç›®å½•
```

### æ­¥éª¤2: å®‰è£…ä¾èµ–

æ ¹æ®READMEï¼Œéœ€è¦è¿è¡Œï¼š
```bash
cd FinCast-fts
bash env_setup.sh
bash dep_install.sh
```

æˆ–è€…æ‰‹åŠ¨å®‰è£…ï¼ˆæ ¹æ®é¡¹ç›®éœ€æ±‚ï¼‰ï¼š
```bash
pip install transformers>=4.30.0
pip install einops
pip install accelerate
```

### æ­¥éª¤3: ä½¿ç”¨æˆ‘ä»¬çš„é›†æˆä»£ç 

æˆ‘å·²ç»ä¸ºä½ åˆ›å»ºäº†é›†æˆä»£ç ï¼ŒåŒ…æ‹¬ï¼š

1. **`src/models/foundation_models.py`**: FinCastæ¨¡å‹å°è£…ç±»
2. **`config.py`**: FinCastConfigé…ç½®ç±»
3. **`main.py`**: æ”¯æŒ `--model_type FinCast` å‚æ•°

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### å¿«é€Ÿå¼€å§‹

```bash
# åŸºç¡€è®­ç»ƒï¼ˆå¾®è°ƒFinCastï¼‰
python main.py \
    --model_type FinCast \
    --num_epochs 20 \
    --batch_size 16 \
    --learning_rate 1e-4 \
    --experiment_name fincast_finetune

# ä½¿ç”¨LoRAå¾®è°ƒï¼ˆæ¨èï¼ŒèŠ‚çœæ˜¾å­˜ï¼‰
python main.py \
    --model_type FinCast \
    --use_lora \
    --lora_rank 8 \
    --num_epochs 20 \
    --batch_size 16 \
    --learning_rate 1e-3 \
    --experiment_name fincast_lora

# å†»ç»“backboneï¼Œåªè®­ç»ƒé¢„æµ‹å¤´
python main.py \
    --model_type FinCast \
    --freeze_backbone \
    --num_epochs 10 \
    --batch_size 32 \
    --learning_rate 1e-3 \
    --experiment_name fincast_head_only
```

### å‘½ä»¤è¡Œå‚æ•°è¯´æ˜

#### FinCastä¸“ç”¨å‚æ•°

```bash
--model_type FinCast                # ä½¿ç”¨FinCastæ¨¡å‹
--fincast_model_path <path>         # FinCastæƒé‡è·¯å¾„ï¼ˆé»˜è®¤: src/models/FinCast/v1.pthï¼‰
--fincast_config_path <path>        # FinCasté…ç½®æ–‡ä»¶è·¯å¾„
--freeze_backbone                   # å†»ç»“é¢„è®­ç»ƒçš„backbone
--use_lora                          # ä½¿ç”¨LoRAå¾®è°ƒ
--lora_rank 8                       # LoRAç§©ï¼ˆé»˜è®¤8ï¼‰
--lora_alpha 16                     # LoRA alphaï¼ˆé»˜è®¤16ï¼‰
--lora_dropout 0.1                  # LoRA dropoutï¼ˆé»˜è®¤0.1ï¼‰
```

#### æ ‡å‡†è®­ç»ƒå‚æ•°

```bash
--lookback_window 60                # è¾“å…¥çª—å£é•¿åº¦
--forecast_horizon 5                # é¢„æµ‹æ­¥æ•°
--num_epochs 20                     # è®­ç»ƒè½®æ•°
--batch_size 16                     # æ‰¹å¤§å°
--learning_rate 1e-4                # å­¦ä¹ ç‡ï¼ˆFinCastå»ºè®®1e-4åˆ°1e-5ï¼‰
--weight_decay 1e-5                 # æƒé‡è¡°å‡
--eval_interval 50                  # æµ‹è¯•é›†è¯„ä¼°é—´éš”
```

## ğŸ“Š å®éªŒé…ç½®å»ºè®®

### é…ç½®1: å…¨é‡å¾®è°ƒï¼ˆæ˜¾å­˜å……è¶³ï¼‰

```bash
python main.py \
    --model_type FinCast \
    --lookback_window 60 \
    --forecast_horizon 5 \
    --num_epochs 50 \
    --batch_size 16 \
    --learning_rate 1e-5 \
    --weight_decay 1e-6 \
    --eval_interval 100 \
    --experiment_name fincast_full_finetune
```

**ç‰¹ç‚¹**:
- æ›´æ–°æ‰€æœ‰å‚æ•°
- éœ€è¦è¾ƒå¤§æ˜¾å­˜ï¼ˆå»ºè®®16GB+ï¼‰
- è®­ç»ƒæ—¶é—´è¾ƒé•¿
- å¯èƒ½è·å¾—æœ€ä½³æ€§èƒ½

### é…ç½®2: LoRAå¾®è°ƒï¼ˆæ¨èï¼‰

```bash
python main.py \
    --model_type FinCast \
    --use_lora \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0.1 \
    --lookback_window 60 \
    --forecast_horizon 5 \
    --num_epochs 30 \
    --batch_size 32 \
    --learning_rate 1e-3 \
    --eval_interval 100 \
    --experiment_name fincast_lora_r8
```

**ç‰¹ç‚¹**:
- åªè®­ç»ƒå°‘é‡å‚æ•°ï¼ˆ~1%ï¼‰
- æ˜¾å­˜éœ€æ±‚å°ï¼ˆ8GBå¯è¿è¡Œï¼‰
- è®­ç»ƒé€Ÿåº¦å¿«
- æ€§èƒ½æ¥è¿‘å…¨é‡å¾®è°ƒ

### é…ç½®3: ä»…å¾®è°ƒé¢„æµ‹å¤´ï¼ˆå¿«é€ŸéªŒè¯ï¼‰

```bash
python main.py \
    --model_type FinCast \
    --freeze_backbone \
    --num_epochs 10 \
    --batch_size 64 \
    --learning_rate 1e-3 \
    --eval_interval 50 \
    --experiment_name fincast_head_only
```

**ç‰¹ç‚¹**:
- ä»…è®­ç»ƒæœ€åçš„é¢„æµ‹å±‚
- æ˜¾å­˜éœ€æ±‚æœ€å°
- è®­ç»ƒæœ€å¿«
- é€‚åˆå¿«é€ŸéªŒè¯æ•ˆæœ

## ğŸ” æ¨¡å‹æ¶æ„è¯´æ˜

FinCastæ¨¡å‹åŒ…å«ä»¥ä¸‹ç»„ä»¶ï¼š

```python
FinCastModel
â”œâ”€â”€ Embedding Layer          # è¾“å…¥åµŒå…¥
â”œâ”€â”€ Transformer Blocks       # å¤šå±‚Transformerï¼ˆå¸¦MoEï¼‰
â”‚   â”œâ”€â”€ Self-Attention
â”‚   â”œâ”€â”€ MoE Feed-Forward     # æ··åˆä¸“å®¶
â”‚   â””â”€â”€ Layer Norm
â”œâ”€â”€ Output Projection        # è¾“å‡ºæŠ•å½±
â””â”€â”€ Prediction Head          # é¢„æµ‹å¤´ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
```

### å¾®è°ƒç­–ç•¥å¯¹æ¯”

| ç­–ç•¥ | è®­ç»ƒå‚æ•° | æ˜¾å­˜éœ€æ±‚ | è®­ç»ƒé€Ÿåº¦ | æ€§èƒ½ |
|------|----------|----------|----------|------|
| å…¨é‡å¾®è°ƒ | 100% | é«˜ï¼ˆ16GB+ï¼‰ | æ…¢ | æœ€ä½³ |
| LoRAå¾®è°ƒ | ~1% | ä¸­ï¼ˆ8GBï¼‰ | ä¸­ | æ¥è¿‘å…¨é‡ |
| ä»…é¢„æµ‹å¤´ | < 0.1% | ä½ï¼ˆ4GBï¼‰ | å¿« | åŸºçº¿ |

## ğŸ“ˆ é¢„æœŸæ•ˆæœ

åŸºäºFinCastè®ºæ–‡ç»“æœï¼Œåœ¨é‡‘èæ—¶åºæ•°æ®ä¸Šï¼š

- **Zero-shot**: MAE ~0.05-0.08ï¼ˆæ— å¾®è°ƒï¼‰
- **Few-shot** (10æ ·æœ¬): MAE ~0.03-0.05
- **Full Fine-tune**: MAE ~0.02-0.03

ä½ çš„æ•°æ®ï¼ˆ296åªè‚¡ç¥¨ï¼Œ431å¤©ï¼‰åº”è¯¥èƒ½è¾¾åˆ°æˆ–è¶…è¿‡Few-shotæ€§èƒ½ã€‚

## ğŸ› å¸¸è§é—®é¢˜

### Q1: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

**A**: å°è¯•ä»¥ä¸‹æ–¹æ³•ï¼š
1. ä½¿ç”¨LoRAå¾®è°ƒ: `--use_lora`
2. å‡å°batch size: `--batch_size 8`
3. å‡å°è¾“å…¥çª—å£: `--lookback_window 30`
4. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯: åœ¨configä¸­è®¾ç½® `gradient_accumulation_steps=4`

### Q2: å¦‚ä½•é€‰æ‹©å­¦ä¹ ç‡ï¼Ÿ

**A**: æ ¹æ®å¾®è°ƒç­–ç•¥ï¼š
- å…¨é‡å¾®è°ƒ: `1e-5` åˆ° `1e-4`
- LoRAå¾®è°ƒ: `1e-3` åˆ° `5e-4`
- ä»…é¢„æµ‹å¤´: `1e-3` åˆ° `1e-2`

### Q3: FinCastæ¨¡å‹æ‰¾ä¸åˆ°æƒé‡æ–‡ä»¶ï¼Ÿ

**A**: ç¡®ä¿æƒé‡æ–‡ä»¶åœ¨æ­£ç¡®ä½ç½®ï¼š
```bash
ls src/models/FinCast/v1.pth

# å¦‚æœä¸å­˜åœ¨ï¼Œæ‰‹åŠ¨ä¸‹è½½
# æˆ–ä½¿ç”¨ --fincast_model_path æŒ‡å®šè·¯å¾„
python main.py --model_type FinCast --fincast_model_path /path/to/v1.pth
```

### Q4: å¦‚ä½•å¯¹æ¯”FinCastå’Œä¼ ç»Ÿæ¨¡å‹ï¼Ÿ

**A**: è¿è¡Œæ‰¹é‡å®éªŒï¼š
```bash
# ä¼ ç»ŸLSTM
python main.py --model_type LSTM --experiment_name exp_lstm

# FinCastå¾®è°ƒ
python main.py --model_type FinCast --use_lora --experiment_name exp_fincast_lora

# å¯¹æ¯”ç»“æœ
python -c "
import pandas as pd
lstm_metrics = pd.read_csv('results/exp_lstm/metrics_comparison.csv', index_col=0)
fincast_metrics = pd.read_csv('results/exp_fincast_lora/metrics_comparison.csv', index_col=0)

print('LSTMç»“æœ:')
print(lstm_metrics)
print('\nFinCastç»“æœ:')
print(fincast_metrics)
"
```

## ğŸ“ ä¸‹ä¸€æ­¥è®¡åˆ’

1. **è¿è¡Œå¿«é€Ÿæµ‹è¯•**:
   ```bash
   python main.py --model_type FinCast --num_epochs 2 --batch_size 16
   ```

2. **å¯¹æ¯”åŸºçº¿æ¨¡å‹**:
   - LSTM: `python main.py --model_type LSTM --num_epochs 20`
   - FinCast: `python main.py --model_type FinCast --use_lora --num_epochs 20`

3. **è°ƒä¼˜LoRAå‚æ•°**:
   - å°è¯•ä¸åŒrank: 4, 8, 16, 32
   - å°è¯•ä¸åŒalpha: 8, 16, 32

4. **æ’°å†™è®ºæ–‡**:
   - å¯¹æ¯”Zero-shotã€Few-shotã€Full Fine-tune
   - åˆ†æFinCaståœ¨è‚¡ç¥¨é¢„æµ‹ä¸Šçš„ä¼˜åŠ¿

## ğŸ“ è®ºæ–‡å®éªŒå»ºè®®

### å®éªŒ1: Zero-shot vs Fine-tuned

```bash
# Zero-shotï¼ˆåŠ è½½é¢„è®­ç»ƒæƒé‡ï¼Œä¸è®­ç»ƒï¼‰
python main.py --model_type FinCast --num_epochs 0 --experiment_name fincast_zeroshot

# Fine-tuned
python main.py --model_type FinCast --use_lora --num_epochs 20 --experiment_name fincast_finetuned
```

### å®éªŒ2: LoRA Rankæ¶ˆèå®éªŒ

```bash
for rank in 4 8 16 32; do
    python main.py \
        --model_type FinCast \
        --use_lora \
        --lora_rank $rank \
        --num_epochs 20 \
        --experiment_name fincast_lora_r${rank}
done
```

### å®éªŒ3: ä¸ä¼ ç»Ÿæ¨¡å‹å¯¹æ¯”

```bash
# è¿è¡Œæ‰€æœ‰åŸºçº¿
for model in LSTM GRU Transformer; do
    python main.py --model_type $model --num_epochs 20 --experiment_name exp_${model}
done

# FinCast
python main.py --model_type FinCast --use_lora --num_epochs 20 --experiment_name exp_fincast
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- **FinCastè®ºæ–‡**: CIKM 2025ï¼ˆå¾…å‘å¸ƒé“¾æ¥ï¼‰
- **å®˜æ–¹GitHub**: https://github.com/vincent05r/FinCast-fts
- **Hugging Face**: https://huggingface.co/Vincent05R/FinCast
- **æœ¬é¡¹ç›®æ–‡æ¡£**: 
  - `NEW_FEATURES_GUIDE.md`: æ–°åŠŸèƒ½ä½¿ç”¨æŒ‡å—
  - `CHECKPOINT_AND_MONITORING.md`: Checkpointå’Œç›‘æ§
  - `QUICK_TEST.md`: å¿«é€Ÿæµ‹è¯•

---

**å‡†å¤‡å¥½å¼€å§‹äº†ï¼** ğŸš€

ç°åœ¨éœ€è¦çš„æ­¥éª¤ï¼š
1. ç­‰æˆ‘å®Œæˆä»£ç å®ç°
2. å®‰è£…FinCastä¾èµ–
3. è¿è¡Œå¿«é€Ÿæµ‹è¯•
4. æŸ¥çœ‹resultsç›®å½•ä¸‹çš„ç»“æœ
